# -*- coding: utf-8 -*-
"""Lorenz_Attractor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ytc8dfoHv8RNy93ZI3J_Zip9BxiID_RB

<a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Licencia Creative Commons" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title"></span> The following notes written by  <span xmlns:cc="http://creativecommons.org/ns#" property="cc:attributionName"> Arturo Fredes Cáceres and Sergio Gutiérrez Rodrigo (<sergut@unizar.es>) </span>. Distributed under  <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">License Creative Commons Atribución-NoComercial-CompartirIgual 4.0 Internacional</a>

# Import the main libraries
"""

import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

"""# Mount google Drive"""

# Commented out IPython magic to ensure Python compatibility.
#Montar el drive e ir a la carpeta
from google.colab import drive
drive.mount('/content/drive',force_remount=True)
# %cd /content/drive/MyDrive/tfg_arturo_fredes/
# %ls -lht

# Commented out IPython magic to ensure Python compatibility.
#cuenta edelvives
from google.colab import drive
drive.mount('/content/drive')
# %cd /content/drive/MyDrive/tfg_arturo_fredes

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/drive/MyDrive/TFG_Físicas_2022-23/tfg_arturo_fredes/
# %ls -lht

"""#Generating and saving Data

## Generating Data, Lorenz Attractor
"""

def rungekutta4(f, x0, t):
    n = len(t)
    x = np.zeros((n, len(x0)))
    x[0] = x0
    for i in range(n - 1):
        h = t[i+1] - t[i]
        k1 = f(x[i])
        k2 = f(x[i] + k1 * h / 2.)
        k3 = f(x[i] + k2 * h / 2.)
        k4 = f(x[i] + k3 * h)
        x[i+1] = x[i] + (h / 6.) * (k1 + 2*k2 + 2*k3 + k4)
    return x

#Function that returns the equations of Lorentz Attractor
def Lorenz_2(x):
  a=10
  b=28
  c=8/3
  return np.array([a*(x[1]-x[0]),x[0]*(b-x[2])-x[1],x[0]*x[1]-c*x[2]])

def generate_data_2(n_steps, batch_size):
  data=[]
  targets=[]
  t = np.linspace(0, 100, n_steps)
  for i in range (1,batch_size+1):
    x0 = np.random.uniform(-1.0, 1.0)
    y0 = np.random.uniform(-1.0, 1.0)
    z0 = np.random.uniform(-1.0, 1.0)
    results =  rungekutta4(f=Lorenz_2, x0=[x0, y0,z0],t=t)
    data.append(results[:n_steps])

  return np.array(data)

n_steps=1000 # Diverge si el número de pasos es pequeño
target_len=10
batch_size=500

data=generate_data_2(n_steps,batch_size)

print(data.shape)
print(data[0].shape)

"""## Saving Data (takes too long to generate everytime)"""

file_path = './data_lorenz4.txt'
data_2D = part4.reshape(-1, data.shape[-1])
np.savetxt(file_path, data_2D, fmt='%.8f')

split_point = data[0] // 2

# Split the large array into two parts
part1 = data[:1250]
part2 = data[1250:2500]
part3 = data[2500:3750]
part4 = data[3750:]

"""#Reading and manipulating Data

## Reading Data
"""

# Load the data from the text document
#file_path = '/content/drive/MyDrive/tfg_arturo_fredes/data_lorenz.txt'
file_path = 'data_lorenz.txt'
data = np.loadtxt(file_path, dtype=float)


# Determine the shape of the original 3D array

time_steps=1000 #small data set (def)
rows, cols = data.shape
num_slices = rows // time_steps

# Reshape the 2D array back into a 3D array
data = np.array(data.reshape(num_slices, time_steps, cols))
print(data.shape)



"""## Bigger time window (optional)
With 10000 timesteps it takes too long to train the models, so I will make the time window bigger to have fewer points.
"""

new_time_window = 5
data = data[:, ::new_time_window, :]

data.shape

"""## Shorter Series (optional)"""

data.shape

data = np.reshape(data, (50000, 1000, 3))#reshape each timeseries into 10 timeseries of 10 steps

#shuffle and select 5000 timeseries
np.random.shuffle(data)
data=data[:5000]

"""## Normalizing"""

mean = np.mean(data, axis=(0, 1))
std = np.std(data, axis=(0, 1))

data=(data-mean)/std

"""

## Dividing data
Training, validation and testing"""

#we divide data into training, validation and testing
n_steps=data.shape[1]
target_len=100
train_samples=3500
val_samples=1000
train_data= data[:train_samples,:n_steps-target_len,:]
train_tar=data[:train_samples,n_steps-target_len+1:,:]
val_data= data[train_samples:train_samples+val_samples,:n_steps-target_len,:]
val_tar= data[train_samples:train_samples+val_samples,n_steps-target_len+1:,:]
test_data= data[train_samples+val_samples:,:n_steps-target_len,:]
test_tar= data[train_samples+val_samples:,n_steps-target_len+1:,:]

"""# Forecasting the next timestep

## Baselines
We set the Baseline of our problem.


1.   Predicting the last value --> MSE 0.02
2.   Dense NN ('Adam', 20 Epochs)--> MSE 0.004
"""

#BASELINE1
#we predict the last value from each series

random_int = np.random.randint(0, len(test_tar))
print('MSE: ')
np.mean(tf.keras.losses.mean_squared_error(test_tar[:,0], test_data[:,-1]))

test_data[random_int][-1]

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
random_int=1
# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])*std[0]+mean[0]
y_values = np.array([result[1] for result in test_data[random_int]])*std[1]+mean[1]
z_values = np.array([result[2] for result in test_data[random_int]])*std[2]+mean[2]

x_tar=test_tar[random_int][0][0]*std[0]+mean[0]
y_tar=test_tar[random_int][0][1]*std[1]+mean[1]
z_tar=test_tar[random_int][0][2]*std[2]+mean[2]

x_naive=test_data[random_int][-1][0]*std[0]+mean[0]
y_naive=test_data[random_int][-1][1]*std[1]+mean[1]
z_naive=test_data[random_int][-1][2]*std[2]+mean[2]


fig = plt.figure(figsize=(8,6))
plt.rcParams['font.size'] = 15
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,'y',label='data')
ax.scatter(x_naive,y_naive,z_naive, c='red', label='naive')
ax.scatter(x_tar,y_tar,z_tar, c='black',marker='x', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z', labelpad=10)

ax.legend()

plt.savefig('./naive_lorenz{i}.png'.format(i=random_int))

#BASELINE2
#fully connected dense network with flattened input
model1 = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=[train_data.shape[1], 3]),
tf.keras.layers.Dense(3)
])

model1.summary()
model1.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

model1.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

train_loss=[86.0071,22.4443,15.7068,10.6695,8.4852,7.4005,6.9976,6.1723,5.7931,5.5429,5.2150, 5.2621,5.4433,4.1499,4.1844,4.4689,4.9883,5.0311, 4.8030,3.6549]
val_loss=[27.1010,16.4008,11.5704,8.3480,9.3406,6.4720,10.4948,6.8049,9.0846,5.2156,3.7501,5.6798,8.0913,2.8925,6.2985,5.5973,4.3464,8.2140,3.6560,3.9278]

epochs = range(1, len(train_loss) + 1)



# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('DNN')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, len(test_tar))

# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model1.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DNN=pred[0][0]
y_DNN=pred[0][1]
z_DNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,color='yellow',label='data')
ax.scatter(x_DNN,y_DNN,z_DNN,'x' ,c='black', label='DNN')
ax.scatter(x_tar,y_tar,z_tar, c='red', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""## DNN"""

model2 = tf.keras.models.Sequential([
tf.keras.layers.Flatten(input_shape=[train_data.shape[1], 3]),
tf.keras.layers.Dense(64),
tf.keras.layers.Dense(64),
tf.keras.layers.Dense(3)
])

model2.summary()

model2.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

history=model2.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

model2.save('./Lorenz_short_model_DNN_1000_norm.h5',save_format='h5')

train_loss = history.history['loss']
epochs = range(1, len(train_loss) + 1)

# Extract the validation loss values
val_loss = history.history['val_loss']

# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Deep DNN')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

#random_int = np.random.randint(0, len(test_tar))
random_int=0
# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model2.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]
y_DRNN=pred[0][1]
z_DRNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data')
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='red', label='Deep DNN')
ax.scatter(x_tar,y_tar,z_tar, c='yellow', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""## Deep RNN
stack multiple layers of cells
MSE--> 0.0027 we beat the baseline!
"""

model3 = tf.keras.models.Sequential([
tf.keras.layers.SimpleRNN(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.SimpleRNN(64, return_sequences=True),
tf.keras.layers.SimpleRNN(3)
])

model3.summary()

model3.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

model3.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

model3.save('./Lorenz_short_model_DeepRNN_1000_norm.h5',save_format='h5')

train_loss = [247.7636, 240.6221, 240.2091, 240.1031, 240.0519, 240.0198, 239.9933, 239.9704, 239.9439, 239.9158, 239.8844, 239.8520, 239.8312, 239.8187, 239.8107, 239.8055, 239.8016, 239.7981, 239.7953, 239.7932]

val_loss = [234.4393, 233.3776, 233.2153, 233.1483, 233.1068, 233.0773, 233.0515, 233.0264, 232.9977, 232.9716, 232.9431, 232.9200, 232.9060, 232.8962, 232.8898, 232.8857, 232.8833, 232.8773, 232.8778, 232.8714]

epochs = range(1, len(train_loss) + 1)



# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Deep RNN')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, len(test_tar))

# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model3.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]
y_DRNN=pred[0][1]
z_DRNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data')
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='red', label='Deep RNN')
ax.scatter(x_tar,y_tar,z_tar, c='yellow', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""Make sure to set return_sequences=True for all recurrent layers
(except the last one, if you only care about the last output). If you
don’t, they will output a 2D array (containing only the output of
the last time step) instead of a 3D array (containing outputs for all
time steps), and the next recurrent layer will complain that you are
not feeding it sequences in the expected 3D format.

The last layer is not ideal:


1.   Cannot choose activation function
2.   it must have a single unit because we want to forecast
a univariate time series, and this means we must have a single output value per
time step. However, having a single unit means that the hidden state is just a single
number. That’s really not much, and it’s probably not that useful; presumably, the
RNN will mostly use the hidden states of the other recurrent layers to carry over all
the information it needs from time step to time step, and it will not use the final layer’s
hidden state very much

We can use a Dense layer to solve this problems. MSE is similar, but it is more efficient.
"""

model4= tf.keras.models.Sequential([
tf.keras.layers.SimpleRNN(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.SimpleRNN(64),
tf.keras.layers.Dense(3)
])

model4.summary()

model4.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

model4.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

model4.save('./Lorenz_short_model_DeepRNN2_1000_norm.h5',save_format='h5')

train_loss = [211.4714, 151.8391, 126.2590, 107.3105, 92.1312, 79.8090, 69.7872, 61.6952, 55.1724, 49.9070, 45.5171, 40.2810, 33.5081, 28.3590, 24.4931, 21.2342, 18.5056, 16.1945, 14.2245, 12.5433]

val_loss = [164.0393, 132.7289, 111.9095, 95.5795, 82.3515, 71.6422, 63.0261, 56.0643, 50.4994, 45.9335, 41.5933, 35.8337, 29.2239, 25.0277, 21.6257, 18.6944, 16.2763, 14.2216, 12.4693, 10.9803]

epochs = range(1, len(train_loss) + 1)



# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Deep RNN2')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, len(test_tar))

# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model4.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]
y_DRNN=pred[0][1]
z_DRNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data')
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='red', label='Deep RNN2')
ax.scatter(x_tar,y_tar,z_tar, c='yellow', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""##LSTM1"""

model6 = tf.keras.models.Sequential([
tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.LSTM(64),
tf.keras.layers.Dense(3)
])
model6.summary()

model6.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

history=model6.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

model6.save('./Lorenz_short_model_LSTM_1000_norm.h5',save_format='h5')

train_loss = history.history['loss']
epochs = range(1, len(train_loss) + 1)

# Extract the validation loss values
val_loss = history.history['val_loss']

# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Deep LSTM')
plt.legend()
plt.show()

train_loss = [192.8099, 138.4500, 116.2855, 100.0467, 86.8803, 76.3756, 67.3784, 59.0586, 51.2675, 44.6923, 38.5838, 33.4869, 28.7649, 24.7028, 21.3430, 18.4669, 16.0840, 14.0718, 12.3374, 10.8600]

val_loss = [153.8288, 126.1862, 107.8353, 93.3959, 81.5678, 72.1108, 63.7142, 55.4574, 48.8737, 42.0092, 36.4733, 31.5639, 27.0488, 23.3552, 20.2156, 17.6046, 15.4183, 13.5062, 11.9183, 10.5713]
epochs = range(1, len(train_loss) + 1)



# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('LSTM')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, len(test_tar))

# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model6.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]
y_DRNN=pred[0][1]
z_DRNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data')
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='red', label='Deep RNN')
ax.scatter(x_tar,y_tar,z_tar, c='yellow', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""## GRU1

"""

model7 = tf.keras.models.Sequential([
tf.keras.layers.GRU(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.GRU(64),
tf.keras.layers.Dense(3)
])
model7.summary()

model7.compile(loss='mse',
          optimizer='adam',
          metrics=['mae'])

history=model7.fit(train_data, train_tar[:,0,:], epochs=20, validation_data=(val_data, val_tar[:,0,:]))

model7.save('./Lorenz_short_model_GRU_1000_norm.h5',save_format='h5')

train_loss = history.history['loss']
epochs = range(1, len(train_loss) + 1)

# Extract the validation loss values
val_loss = history.history['val_loss']

# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Deep GRU')
plt.legend()
plt.show()

train_loss = [204.5139, 158.2448, 131.8358, 111.2111, 94.3703, 80.5454, 69.1798, 59.9119, 52.3158, 45.9086, 39.9290, 33.7979, 29.1640, 25.2952, 22.0390, 19.2510, 16.8690, 14.8333, 13.0710, 11.5510]

val_loss = [174.7308, 143.8083, 120.8174, 102.2297, 86.9140, 74.3910, 64.1034, 55.7613, 48.9008, 43.1500, 36.5603, 31.3921, 27.2573, 23.7168, 20.7394, 18.1720, 15.9856, 14.1261, 12.5085, 11.1092]

epochs = range(1, len(train_loss) + 1)



# Plot the training loss and validation loss vs. epoch
plt.plot(epochs, train_loss, label='Training Loss')
plt.plot(epochs, val_loss, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('GRU')
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, len(test_tar))

# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])
y_values = np.array([result[1] for result in test_data[random_int]])
z_values = np.array([result[2] for result in test_data[random_int]])

x_tar=test_tar[random_int][0][0]
y_tar=test_tar[random_int][0][1]
z_tar=test_tar[random_int][0][2]

pred=model7.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]
y_DRNN=pred[0][1]
z_DRNN=pred[0][2]

fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data')
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='red', label='Deep RNN')
ax.scatter(x_tar,y_tar,z_tar, c='yellow', label='target')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()

"""##Load a Model and make predictions"""

# Commented out IPython magic to ensure Python compatibility.
# %ls modelos

model = tf.keras.models.load_model('./modelos/Lorenz_short_model_DeepRNN2_1000_norm.h5')

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

#random_int = np.random.randint(0, len(test_tar))
random_int=1
# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])*std[0]+mean[0]
y_values = np.array([result[1] for result in test_data[random_int]])*std[1]+mean[1]
z_values = np.array([result[2] for result in test_data[random_int]])*std[2]+mean[2]

x_tar=test_tar[random_int][0][0]*std[0]+mean[0]
y_tar=test_tar[random_int][0][1]*std[1]+mean[1]
z_tar=test_tar[random_int][0][2]*std[2]+mean[2]

pred=model.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
x_DRNN=pred[0][0]*std[0]+mean[0]
y_DRNN=pred[0][1]*std[1]+mean[1]
z_DRNN=pred[0][2]*std[2]+mean[2]

fig = plt.figure(figsize=(8,6))
plt.rcParams['font.size'] = 15
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,'y',label='data')
ax.scatter(x_tar,y_tar,z_tar, c='red', label='target',zorder=10)
ax.scatter(x_DRNN,y_DRNN,z_DRNN, c='black',marker='x', label='Deep GRU',zorder=11)

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()


plt.savefig('./Lorenz_RNN{i}.png'.format(i=random_int))



"""# Forecasting Several timesteps
We have two options:


1.   Forecast the next step, add it to the timeseriries, and repeat
2.   Use a sequence to sequence network and train it to give all steps at once

## Forecast the next step, add it to the timeseries, and repeat
"""

model1 = tf.keras.models.load_model('./modelos/Lorenz_short_model_LSTM_1000_norm.h5')

#predict
n_pred=100
random_int = np.random.randint(0, len(test_tar))
random_int=16
sequence=test_data[random_int].reshape(1,train_data.shape[1], 3)
for i in range (0,n_pred):
  pred=model1.predict(sequence[:,i:,:]) #predict next timestep with previous 1000 steps
  sequence=np.concatenate((sequence, pred.reshape(1, 1, 3)), axis=1) #add it to the sequence

sequence[0,train_data.shape[1]+1:,:].shape

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int,:train_data.shape[1],:]])*std[0]+mean[0]
y_values = np.array([result[1] for result in test_data[random_int,:train_data.shape[1],:]])*std[1]+mean[1]
z_values = np.array([result[2] for result in test_data[random_int,:train_data.shape[1],:]])*std[2]+mean[2]

# Extract x, y, z values from the targets

x_tar_values = np.array([result[0] for result in test_tar[random_int,:,:]])*std[0]+mean[0]
y_tar_values = np.array([result[1] for result in test_tar[random_int,:,:]])*std[1]+mean[1]
z_tar_values = np.array([result[2] for result in test_tar[random_int,:,:]])*std[2]+mean[2]

#predicted values
x_pred_values = np.array([result[0] for result in sequence[0,train_data.shape[1]+1:,:]])*std[0]+mean[0]
y_pred_values = np.array([result[1] for result in sequence[0,train_data.shape[1]+1:,:]])*std[1]+mean[1]
z_pred_values = np.array([result[2] for result in sequence[0,train_data.shape[1]+1:,:]])*std[2]+mean[2]



fig = plt.figure(figsize=(8,6))
plt.rcParams['font.size'] = 15
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,label='data',c='yellow')
ax.scatter(x_pred_values, y_pred_values, z_pred_values,c='black',marker='x', label='Iterative LSTM')
ax.scatter(x_tar_values, y_tar_values, z_tar_values,c='red' ,label='targets')

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()
plt.savefig('./Lorenz_Iter_LSTM_3D_{i}.png'.format(i=random_int))
print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int]*std+mean,sequence[0,train_data.shape[1]+1:,:]*std+mean)))
print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int,-1,:]*std+mean,sequence[0,-1,:]*std+mean)))

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Create an array for time
time_values = np.arange(len(test_data[random_int]))
time_values_tar = np.arange(900,1000)
# Plot the 3D figure
fig = plt.figure(figsize=(10, 6))
plt.rcParams['font.size'] = 15

# Create a 2D plot for the projection on the x-axis
fig2 = plt.figure(figsize=(10, 4))
plt.plot(time_values, x_values, label='x values', color='blue')
plt.plot(np.arange(900,999), x_tar_values.reshape(99) , 'ro', label='target', color='red')
plt.plot(np.arange(900,999), x_pred_values.reshape(99), 'bx', label='Iterative LSTM', color='black')

plt.xlabel('Time')
plt.ylabel('x')
plt.legend()
plt.savefig('./Lorenz_Iter_LSTM_proj_{i}.png'.format(i=random_int))
plt.show()

n_pred=99
y_pred=[]
for i in range(0,500):
  sequence=test_data[i].reshape(1,test_data.shape[1], 3)
  for j in range (0,n_pred):
    pred=model1.predict(sequence[:,j:,:]) #predict next timestep with previous 1000 steps
    sequence=np.concatenate((sequence, pred.reshape(1, 1, 3)), axis=1) #add it to the sequence
  y_pred.append(sequence[0,test_data.shape[1]:,:])
  print(i)

predictions=np.array(y_pred)
predictions=predictions.reshape((500,99,3))
print(predictions.shape)

file_path = './iterpredLorenz.txt'
data_2D = predictions.reshape(-1, predictions.shape[-1])
np.savetxt(file_path, data_2D, fmt='%.8f')

# Load the data from the text document
file_path = 'iterpredLorenz.txt'
predictions = np.loadtxt(file_path, dtype=float)


# Determine the shape of the original 3D array
#time_steps=10000 #big data set (1 and 2)
time_steps=99 #small data set (def)
rows, cols = predictions.shape
num_slices = rows // time_steps

# Reshape the 2D array back into a 3D array
predictions = np.array(predictions.reshape(num_slices, time_steps, cols))
print(predictions.shape)

print('last_mse: ', tf.reduce_mean(last_time_step_mse(test_tar[:250]*std+mean,predictions[:,:-1,:].reshape((250,99,3))*std+mean)))
print('mse: ', tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[:250]*std+mean,predictions[:,:-1,:].reshape((250,99,3))*std+mean)))

for coord in range (0,3):
  if coord==0:
    title='X'
    a=-15
    b=15
  elif coord==1:
    title='Y'
    a=-20
    b=20
  elif coord==2:
    title='Z'
    a=5
    b=40
  plt.scatter(std[coord]*predictions[:,-1,coord]+mean[coord], std[coord]*test_tar[:,-1,coord]+mean[coord], color='black',marker='x' ,label='Data points')
  x=[a,b]
  y=[a,b]
  plt.plot(x, y, color='red' ,label='x=y', linewidth=3.0)


  plt.xlabel('Predictions')
  plt.ylabel('Targets')

  plt.title(title)
  #plt.legend(loc='upper right')
  plt.rcParams['font.size'] = 20
  plt.show()

print('sequence mse: ', np.mean(tf.keras.losses.mean_squared_error(test_tar[:250]*std+mean, pred*std+mean)))
#print('last step mse: ',np.mean(tf.keras.losses.mean_squared_error(test_tar[:250,-1,:]*std+mean, pred[:,-1,:]*std+mean)))

tf.reduce_mean(tf.keras.losses.mean_squared_error(test_tar[:250]*std+mean, pred*std+mean)).shape

"""## Sequence to vector"""

def last_time_step_mse(y_true,y_pred):
  return tf.keras.metrics.mean_squared_error(y_true[:,-1],y_pred[:,-1])

train_data.shape

train_tar=np.reshape(train_tar,(train_tar.shape[0],3*train_tar.shape[1]))
val_tar=np.reshape(val_tar,(val_tar.shape[0],3*val_tar.shape[1]))

print(train_tar.shape)
print(train_data.shape)

model = tf.keras.models.Sequential([
tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.LSTM(64),
tf.keras.layers.Dense(3*(target_len-1))
])
model.summary()

model.compile(loss='mse', #'mse',#[last_time_step_loss],
          optimizer='adam',
          metrics='mae')

model.fit(train_data, train_tar,
           epochs=20,
           validation_data=(val_data, val_tar))

model.save('./Lorenz_short_model_S-V_LSTM_1000_norm_100.h5',save_format='h5')

model = tf.keras.models.load_model('./modelos/Lorenz_short_model_S-V_LSTM_1000_norm_100.h5')

pred=model.predict(test_data) #make a prediction for a random timeseries
pred=pred.reshape(500,99,3)

print('sequence mse: ', np.mean(tf.keras.losses.mean_squared_error(test_tar*std+mean, pred*std+mean)))
print('last step mse: ',np.mean(tf.keras.losses.mean_squared_error(test_tar[:,-1,:]*std+mean, pred[:,-1,:]*std+mean)))

for coord in range (0,3):
  if coord==0:
    title='X'
    a=-15
    b=15
  elif coord==1:
    title='Y'
    a=-20
    b=20
  elif coord==2:
    title='Z'
    a=5
    b=40
  plt.scatter(std[coord]*pred[:,-2,coord]+mean[coord], std[coord]*test_tar[:,-1,coord]+mean[coord], color='black',marker='x' ,label='Data points')
  x=[a,b]
  y=[a,b]
  plt.plot(x, y, color='red' ,label='x=y', linewidth=3.0)


  plt.xlabel('Predictions')
  plt.ylabel('Targets')

  plt.title(title)
  #plt.legend(loc='upper right')
  plt.rcParams['font.size'] = 20
  plt.show()

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, 500)
random_int=16
# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])*std[0]+mean[0]
y_values = np.array([result[1] for result in test_data[random_int]])*std[1]+mean[1]
z_values = np.array([result[2] for result in test_data[random_int]])*std[2]+mean[2]

x_tar=test_tar[random_int,:,0]*std[0]+mean[0]
y_tar=test_tar[random_int,:,1]*std[1]+mean[1]
z_tar=test_tar[random_int,:,2]*std[2]+mean[2]

pred=model.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
pred=pred.reshape(1,99,3)
x_DRNN=pred[0,:,0]*std[0]+mean[0]
y_DRNN=pred[0,:,1]*std[1]+mean[1]
z_DRNN=pred[0,:,2]*std[2]+mean[2]

fig = plt.figure(figsize=(8,6))
plt.rcParams['font.size'] = 15
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,'y',label='data')
ax.scatter(x_tar,y_tar,z_tar, c='red', label='target',zorder=11)
ax.scatter(x_DRNN[:-1],y_DRNN[:-1],z_DRNN[:-1], c='black',marker='x', label='S-V LSTM',zorder=13)

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()


plt.savefig('./Lorenz_S-VLSTM_3D_{i}.png'.format(i=random_int))

print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int]*std+mean,pred*std+mean)))
print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int,-1,:]*std+mean,pred[:,-1,:]*std+mean)))

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Create an array for time
time_values = np.arange(len(test_data[random_int]))
time_values_tar = np.arange(950,1000)
# Plot the 3D figure
fig = plt.figure(figsize=(10, 6))
plt.rcParams['font.size'] = 15

# Create a 2D plot for the projection on the x-axis
fig2 = plt.figure(figsize=(10, 4))
plt.plot(time_values, x_values, label='x values', color='blue')
plt.plot(np.arange(900,999), x_tar.reshape(99) , 'ro', label='target', color='red')
plt.plot(np.arange(900,999), x_DRNN[:-1].reshape(99), 'bx', label='S-V LSTM', color='black')

plt.xlabel('Time')
plt.ylabel('x')
plt.legend()
plt.savefig('./Lorenz_S-VLSTM_proj_{i}.png'.format(i=random_int))
plt.show()

"""##Use a sequence to sequence network and train it to give all steps at once

### Creating new targets
"""

#now, every target will contain n_steps vectors of dimension 10, each one shifted by 10 steps.
n_steps=1000
target_len=100
Y = np.empty((data.shape[0], n_steps-target_len, target_len,3)) # each target is a sequence of 100D vectors
print(n_steps)
print(Y.shape)
print(data.shape)
for step_ahead in range(1, target_len):
  Y[:, :, step_ahead - 1,:] = data[:, step_ahead:step_ahead + n_steps-target_len, :]

#Dividing the
train_samples=3500
val_samples=1000
#Y_train = Y[:train_samples]
#Y_valid = Y[train_samples:train_samples+val_samples]
Y_test =  Y[train_samples:train_samples+val_samples:]
print(Y_train.shape)

#Reshape targets so we can feed them to the model
print(Y_train.shape)
Y_train=np.reshape(Y_train,(train_samples,n_steps-target_len,3*target_len))
Y_valid=np.reshape(Y_valid,(val_samples,n_steps-target_len,3*target_len))
print(Y_train.shape)

"""### Define the model"""

model15 = tf.keras.models.Sequential([
tf.keras.layers.SimpleRNN(64, return_sequences=True, input_shape=[None, 3]),
tf.keras.layers.SimpleRNN(64, return_sequences=True),
tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(3*target_len))
])
model15.summary()

"""### Define custom loss and metrics"""

def last_time_step_mse(y_true,y_pred):
  return tf.keras.metrics.mean_squared_error(y_true[:,-1],y_pred[:,-1])

"""### Model compile and fit"""

model15.compile(loss='mse', #'mse',#[last_time_step_loss],
          optimizer='adam',
          metrics='mae')

model15.fit(train_data, Y_train,
           epochs=20,
           validation_data=(val_data, Y_valid))

model15.save('./Lorenz_LSTM_seq-seq_1000_100.h5')

"""### Results

"""

model15 = tf.keras.models.load_model('./modelos/Lorenz_LSTM_seq-seq_1000_norm_100.h5')

pred=model15.predict(test_data)
pred=pred.reshape(500,900,100,3)
print(pred.shape)

#mse
print('Series',np.mean(tf.keras.losses.mean_squared_error(test_tar*std+mean, pred[:,-1,:-1,:]*std+mean)))
print('Last step',np.mean(tf.keras.losses.mean_squared_error(test_tar[:,-1,:]*std+mean, pred[:,-1,-2,:]*std+mean)))

#mae
print('Series',np.mean(tf.keras.losses.mean_absolute_error(test_tar*std+mean, pred[:,-1,:-1,:]*std+mean)))
print('Last step',np.mean(tf.keras.losses.mean_absolute_error(test_tar[:,-1,:]*std+mean, pred[:,-1,-2,:]*std+mean)))

for coord in range (0,3):
  if coord==0:
    title='X'
    a=-15
    b=15
  elif coord==1:
    title='Y'
    a=-20
    b=20
  elif coord==2:
    title='Z'
    a=5
    b=40
  plt.scatter(std[coord]*pred[:,-1,-2,coord]+mean[coord], std[coord]*test_tar[:,-1,coord]+mean[coord], color='black',marker='x' ,label='Data points')
  x=[a,b]
  y=[a,b]
  plt.plot(x, y, color='red' ,label='x=y', linewidth=3.0)


  plt.xlabel('Predictions')
  plt.ylabel('Targets')

  plt.title(title)
  #plt.legend(loc='upper right')
  plt.rcParams['font.size'] = 20
  plt.show()

abs_error=abs(test_tar*std+mean- pred[:,-1,:-1,:]*std-mean)
flat_abs_error = abs_error.flatten()
plt.rcParams['font.size'] = 18
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed
plt.hist(flat_abs_error, bins=100, edgecolor='black', alpha=0.7)
plt.title('Histogram of Absolute Errors')
plt.xlabel('Absolute Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.yscale('log')
plt.show()
print(np.mean(flat_abs_error))

#rel_errors = (1/3)*((1 - (std[0] * pred[:,-1,-2,0] + mean[0]) / (std[0] * test_tar[:,-1,0] + mean[0] ))+(1 - (std[1] * pred[:,-1,-2,1] + mean[1]) / (std[1] * test_tar[:,-1,1] + mean[1] ))+(1 - (std[2] * pred[:,-1,-2,2] + mean[2]) / (std[2] * test_tar[:,-1,2] + mean[2] )))
within_range_indices = np.where((abs(std[0]*test_tar[:,-1,0]+mean[0]) > 0.1)&(abs(std[1]*test_tar[:,-1,1]+mean[1]) > 0.1)&(abs(std[2]*test_tar[:,-1,2]+mean[2]) > 0.1))
rel_errors = (1/3)*((1 - (std[0] * pred[within_range_indices,-1,-2,0] + mean[0]) / (std[0] * test_tar[within_range_indices,-1,0] + mean[0] ))+(1 - (std[1] * pred[within_range_indices,-1,-2,1] + mean[1]) / (std[1] * test_tar[within_range_indices,-1,1] + mean[1] ))+(1 - (std[2] * pred[within_range_indices,-1,-2,2] + mean[2]) / (std[2] * test_tar[within_range_indices,-1,2] + mean[2] )))
print(np.mean(abs(rel_errors)))
flat_rel_errors = abs(rel_errors).flatten()
plt.rcParams['font.size'] = 18
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed
plt.hist(abs(flat_rel_errors)*100, bins=100, edgecolor='black', alpha=0.7)
plt.title('Histogram of Relative Errors')
plt.xlabel('Relative Error')
plt.ylabel('Frequency')
plt.grid(True)
plt.yscale('log')
plt.show()
print(np.max(rel_errors))
print(test_tar[within_range_indices].shape)

#Calculating relative errors for the last timestep of each coordinate
c=0#coordinate
rel_errors_x = (1 - (std[c] * pred[:,-1,-2,c] + mean[c]) / (std[c] * test_tar[:,-1,c] + mean[c] ))
print(np.mean(abs(rel_errors_x)))
within_range_indices = np.where((std[c]*test_tar[:,-1,c]+mean[c]) > 1)
rel_errors_x = (1 - (std[c] * pred[within_range_indices,-1,-2,c] + mean[c]) / (std[c] * test_tar[within_range_indices,-1,c] + mean[c] ))
print(np.mean(abs(rel_errors_x)))

within_range_indices = np.where((flat_rel_errors*100 <= 10))
print(' Falla por <=10 %s: ',len(within_range_indices[0]))

within_range_indices = np.where((flat_rel_errors*100 <= 25))
print(' Falla por <=25 %: ',len(within_range_indices[0]))

within_range_indices = np.where((flat_rel_errors*100 <= 50))
print(' Falla por <=50 %: ',len(within_range_indices[0]))

within_range_indices = np.where((flat_rel_errors*100 <= 100))
print(' Falla por <=100 %: ',len(within_range_indices[0]))

within_range_indices = np.where((flat_rel_errors*100 <= 200))
print(' Falla por <=200 %: ',len(within_range_indices[0]))


within_range_indices = np.where((flat_rel_errors*100 <= 500))
print(' Falla por <=500 %: ',len(within_range_indices[0]))

within_range_indices = np.where((flat_rel_errors*100 <= 1000))
print(' Falla por <=1000 %: ',len(within_range_indices[0]))


within_range_indices = np.where((flat_rel_errors*100 > 10) )
print('  ',np.mean(abs(test_tar[within_range_indices,-1,:]*std+mean)))

count_greater_than_100 = np.sum(rel_errors < -1000)

print("Number of relative error values greater than 100:", count_greater_than_100)
valid_indices = rel_errors <= 100
valid_rel_errors = rel_errors[valid_indices]
valid_indices = valid_rel_errors >= -100
# Select only the valid relative error values
valid_rel_errors = valid_rel_errors[valid_indices]
flat_rel_errors = valid_rel_errors.flatten()*100
plt.rcParams['font.size'] = 18
plt.figure(figsize=(10, 6))  # Adjust the figure size as needed
plt.hist(flat_rel_errors, bins=100, edgecolor='black', alpha=0.7)
plt.title('Histogram of Relative Errors')
plt.xlabel('Relative Error')
plt.ylabel('Frequency')
plt.grid(True)
#plt.xscale('log')
plt.show()
print(np.mean(valid_rel_errors)*100)
print(len(valid_rel_errors))

"""### Plotting predictions vs true values"""

import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D

random_int = np.random.randint(0, 500)
random_int=16
# Extract x, y, z values from the results
x_values = np.array([result[0] for result in test_data[random_int]])*std[0]+mean[0]
y_values = np.array([result[1] for result in test_data[random_int]])*std[1]+mean[1]
z_values = np.array([result[2] for result in test_data[random_int]])*std[2]+mean[2]

x_tar=test_tar[random_int,:,0]*std[0]+mean[0]
y_tar=test_tar[random_int,:,1]*std[1]+mean[1]
z_tar=test_tar[random_int,:,2]*std[2]+mean[2]

pred=model15.predict(test_data[random_int].reshape(1,train_data.shape[1], 3)) #make a prediction for a random timeseries
pred=pred.reshape(1,900,100,3)
x_DRNN=pred[0,-1,:,0]*std[0]+mean[0]
y_DRNN=pred[0,-1,:,1]*std[1]+mean[1]
z_DRNN=pred[0,-1,:,2]*std[2]+mean[2]

fig = plt.figure(figsize=(8,6))
plt.rcParams['font.size'] = 15
ax = fig.add_subplot(1,1,1, projection='3d')

axis_lim=20
ax.plot(x_values, y_values, z_values,'y',label='data')
ax.scatter(x_tar,y_tar,z_tar, c='red', label='target',zorder=10)
ax.scatter(x_DRNN[:-1],y_DRNN[:-1],z_DRNN[:-1], c='black',marker='x', label='S-S LSTM',zorder=12)

ax.set_xlim3d(-axis_lim, axis_lim);
ax.set_ylim3d(-axis_lim, axis_lim);
ax.set_zlim3d(0, 2*axis_lim);
# Set labels for x, y, and z axes
ax.set_xlabel('x')
ax.set_ylabel('y')
ax.set_zlabel('z')

ax.legend()


plt.savefig('./Lorenz_S-SLSTM_3D_{i}.png'.format(i=random_int))
print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int]*std+mean,pred[:,-1,:-1,:]*std+mean)))
print(tf.reduce_mean(tf.keras.metrics.mean_squared_error(test_tar[random_int,-1,:]*std+mean,pred[:,-1,-2,:]*std+mean)))

import numpy as np
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D


# Create an array for time
time_values = np.arange(len(test_data[random_int]))
time_values_tar = np.arange(900,1000)
# Plot the 3D figure
fig = plt.figure(figsize=(10, 6))
plt.rcParams['font.size'] = 15

# Create a 2D plot for the projection on the x-axis
fig2 = plt.figure(figsize=(10, 4))
plt.plot(time_values, x_values, label='x values', color='blue')
plt.plot(np.arange(900,999), x_tar.reshape(99) , 'ro', label='target', color='red')
plt.plot(np.arange(900,999), x_DRNN[:-1].reshape(99), 'bx', label='S-S LSTM', color='black')

plt.xlabel('Time')
plt.ylabel('x')
plt.legend()
plt.savefig('./Lorenz_S-SLSTM_proj_{i}.png'.format(i=random_int))
plt.show()